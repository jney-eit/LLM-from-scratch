

load_model: False
model_load_name: "simple_gpt_GQA"

save_model: True
model_save_name: "simple_gpt_GQA"

context_size: 256
batch_size: 64
loss_function: "CrossEntropy"
optimizer: "AdamW"
learning_rate: 3.0e-4
train_iters: 2000   #5000
eval_iters: 100
d_model: 384
num_heads: 6
num_layers: 6
dropout: 0.0 #0.2
do_use_rotary_emb: True
num_kv_heads: 2